# -*- coding: utf-8 -*-
"""code_propre.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/188hPeP2g7wpuJoRhiJrFwpIyojNANzJm
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler

# Chargement des données
housing = fetch_california_housing()
df = pd.DataFrame(housing.data, columns=housing.feature_names)
df["MedHouseVal"] = housing.target

# Visualisation simple des distributions
df.hist(figsize=(12, 10), bins=30)
plt.tight_layout()
plt.show()

# Normalisation des données
scaler = StandardScaler()
df_normaliser = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

# Aperçu
print(df_normaliser.describe())

# Installer folium si nécessaire
!pip install folium branca scikit-learn pandas
import folium
import branca.colormap as cm
from sklearn.datasets import fetch_california_housing
from IPython.display import display
# Centre de la carte (moyenne des coordonnées du dataset)
center = [df["Latitude"].mean(), df["Longitude"].mean()]
# Colormap bleu -> rouge
vmin, vmax = df["MedHouseVal"].min(), df["MedHouseVal"].max()
colormap = cm.LinearColormap(
    colors=["blue", "cyan", "yellow", "orange", "red"],
    vmin=vmin,
    vmax=vmax
)
colormap.caption = "Valeur médiane des maisons ($100,000s)"
# Carte Folium
m = folium.Map(location=center, zoom_start=6, tiles="OpenStreetMAp")
# Points colorés
for _, row in df.sample(frac=0.3, random_state=42).iterrows():
    folium.CircleMarker(
        location=[row["Latitude"], row["Longitude"]],
        radius=3,
        fill=True,
        fill_color=colormap(row["MedHouseVal"]),
        fill_opacity=0.6,
        weight=0
    ).add_to(m)

# Ajouter la légende
m.add_child(colormap)
# Affichage
display(m)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# --- On part de ton DataFrame df déjà chargé ---
# df = pd.DataFrame(housing.data, columns=housing.feature_names)
# df["MedHouseVal"] = housing.target

# ===============================
# 1. Graphiques originaux pour l’EDA (2.4)
# ===============================

# --- Hexbin plot : densité sur Latitude / Longitude ---
plt.figure(figsize=(8,6))
plt.hexbin(df["Longitude"], df["Latitude"], C=df["MedHouseVal"], gridsize=50, cmap="viridis", mincnt=1)
plt.colorbar(label="MedHouseVal")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Distribution des prix des maisons selon la localisation (Hexbin)")
plt.show()

# --- Radial plot : prix moyen par tranche d'âge des maisons ---
df["HouseAge_bin"] = pd.cut(df["HouseAge"], bins=8)
age_means = df.groupby("HouseAge_bin")["MedHouseVal"].mean()

theta = np.linspace(0, 2*np.pi, len(age_means), endpoint=False)
radii = age_means.values
width = 2*np.pi / len(age_means)

plt.figure(figsize=(7,7))
ax = plt.subplot(111, polar=True)
bars = ax.bar(theta, radii, width=width, bottom=0.0, color=plt.cm.viridis(radii/radii.max()))
ax.set_xticks(theta)
ax.set_xticklabels([str(bin) for bin in age_means.index], fontsize=8)
plt.title("Prix moyen par tranche d'âge des maisons (Radial plot)")
plt.show()

# ===============================
# 2. Multifactoriel : interaction de plusieurs variables (2.5)
# ===============================

# --- Heatmap des corrélations ---
plt.figure(figsize=(10,8))
corr = df.corr()
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Matrice de corrélations entre variables")
plt.show()

# --- Scatter plot avec 3 variables : MedInc, HouseAge et MedHouseVal ---
plt.figure(figsize=(8,6))
sns.scatterplot(
    data=df,
    x="MedInc", y="MedHouseVal",
    hue="HouseAge", palette="viridis", alpha=0.7
)
plt.title("Prix des maisons vs revenu médian, coloré par âge des maisons")
plt.xlabel("MedInc")
plt.ylabel("MedHouseVal")
plt.colorbar(label="HouseAge")  # si tu veux une vraie colorbar, matplotlib ou plotly
plt.show()

# --- Pairplot coloré par quartile de MedHouseVal ---
df["ValCat"] = pd.qcut(df["MedHouseVal"], 6, labels=False)
sns.pairplot(
    data=df,
    vars=["MedInc", "HouseAge", "AveRooms", "AveBedrms", "Latitude", "Longitude"],
    hue="ValCat",
    diag_kind="kde",
    corner=True,
    palette="viridis"
)
plt.suptitle("Relations multifactorielle avec MedHouseVal", y=1.02)
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
import numpy as np

# ===============================
# 1. Séparation X / y
# ===============================
X = df.drop(columns="MedHouseVal")
y = df["MedHouseVal"]

# ===============================
# 2. Sélection des variables
#    via régression linéaire normalisée
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=29
)

model = make_pipeline(StandardScaler(), LinearRegression())
model.fit(X_train, y_train)

# Importance des variables
coefs = pd.Series(
    model.named_steps["linearregression"].coef_,
    index=X.columns
)

variables_selectionnees = (
    coefs.abs()
    .sort_values(ascending=False)
    .head(6)
    .index
    .tolist()
)

print("Variables sélectionnées :", variables_selectionnees)

# ===============================
# 3. Pairplot (ggpairs-like)
# ===============================
df_plot = df.copy()

# Catégorisation de la variable cible pour la couleur
df_plot["ValCat"] = pd.qcut(df_plot["MedHouseVal"], 6, labels=False)

sns.pairplot(
    data=df_plot,
    vars=variables_selectionnees,
    hue="ValCat",
    diag_kind="kde",
    corner=True,
    palette="viridis"
)

plt.suptitle(
    "Relations entre variables sélectionnées et prix des maisons en Californie",
    y=1.02
)
plt.show()

subset = df_normaliser[[
    "MedInc", "HouseAge", "AveRooms", "AveBedrms",
    "Population", "AveOccup", "Latitude", "Longitude", "MedHouseVal"
]]

plt.figure(figsize=(10, 8))
sns.heatmap(subset.corr(numeric_only=True), annot=True, cmap="viridis", fmt=".2f")
plt.title("Corrélations")
plt.show()



from scipy import stats

# --- Optionnel : transformation log si la distribution est très asymétrique ---
# housing_data['MedHouseVal_log'] = np.log(housing_data['MedHouseVal'])

# Histogramme avec courbe normale
sns.histplot(df_normaliser['MedHouseVal'], kde=True, stat="density", color="skyblue")
plt.title("Distribution de MedHouseVal")
plt.xlabel("MedHouseVal")
plt.ylabel("Densité")
plt.show()

# QQ-plot pour vérifier la normalité
plt.figure(figsize=(6,6))
stats.probplot(df_normaliser['MedHouseVal'], plot=plt)
plt.title("QQ-plot de MedHouseVal")
plt.show()

import numpy as np
import pandas as pd
from sklearn.utils import resample
from sklearn.metrics import mean_squared_error

X = df_normaliser.drop(columns=["MedHouseVal"])
y = df_normaliser["MedHouseVal"]

# fonction bootstrap
def bootstrap_evaluation(model, X, y, n_bootstrap=100, random_state=0):
    rng = np.random.RandomState(random_state)
    scores = []
    for i in range(n_bootstrap):
        # échantillon avec remise
        X_resampled, y_resampled = resample(X, y, replace=True, random_state=rng)
        model.fit(X_resampled, y_resampled)
        y_pred = model.predict(X)
        mse = mean_squared_error(y, y_pred)
        scores.append(mse)
    return np.array(scores)

# exemple avec un modèle linéaire simple
from sklearn.linear_model import LinearRegression
model = LinearRegression()

scores = bootstrap_evaluation(model, X, y, n_bootstrap=200)
print("MSE moyen bootstrap :", np.mean(scores))
print("IC 95% bootstrap :", np.percentile(scores, [2.5, 97.5]))


# Distribution des scores bootstrap
sns.histplot(scores, bins=30, kde=True)
plt.axvline(np.mean(scores), linestyle="--", label="MSE moyenne")
plt.axvline(np.percentile(scores, 2.5), linestyle=":", label="IC 2.5%")
plt.axvline(np.percentile(scores, 97.5), linestyle=":", label="IC 97.5%")

plt.title("Distribution bootstrap de la MSE (régression linéaire)")
plt.xlabel("MSE")
plt.ylabel("Fréquence")
plt.legend()
plt.show()

# ===============================
# Détection d'anomalies
# ===============================

# 1. Valeurs manquantes
print("Valeurs manquantes par colonne :")
print(df.isnull().sum())

# 2. Outliers via IQR
def detect_outliers_iqr(df, columns):
    outliers = {}
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        mask = (df[col] < lower) | (df[col] > upper)
        outliers[col] = df[col][mask].index.tolist()
    return outliers

numerical_cols = df.columns.tolist()
numerical_cols.remove("MedHouseVal")  # cible non incluse
outliers_dict = detect_outliers_iqr(df, numerical_cols)
for col, idx in outliers_dict.items():
    print(f"{col}: {len(idx)} outliers détectés")

# 3. Boxplots pour visualiser
plt.figure(figsize=(12,6))
sns.boxplot(data=df[numerical_cols])
plt.title("Boxplots pour détection visuelle d'anomalies")
plt.xticks(rotation=45)
plt.show()

from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# on sépare en train / test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "LinReg": LinearRegression(),
    "Ridge": Ridge(alpha=1.0),
    "Lasso": Lasso(alpha=0.1),
    "RF": RandomForestRegressor(n_estimators=100, random_state=42),
    "GB": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
}

results = {}
for name, m in models.items():
    m.fit(X_train, y_train)
    y_pred = m.predict(X_test)
    results[name] = {
        "r2": r2_score(y_test, y_pred),
        "mse": mean_squared_error(y_test, y_pred)
    }


res_df = pd.DataFrame(results).T
print(res_df)
res_df[["r2", "mse"]].plot(kind="bar", subplots=True, layout=(1,2), figsize=(10,4))
plt.show()

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt

# ===============================
# 1. Train / test split
# ===============================
X = df_normaliser.drop(columns="MedHouseVal")
y = df_normaliser["MedHouseVal"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ===============================
# 2. GridSearch Random Forest
# ===============================
rf = RandomForestRegressor(random_state=42)

param_grid = {
    "n_estimators": [100, 200],
    "max_depth": [None, 10],
    "min_samples_split": [2, 5],
    "min_samples_leaf": [1, 2]
}

grid = GridSearchCV(
    rf,
    param_grid,
    cv=5,
    scoring="r2",
    n_jobs=-1
)

grid.fit(X_train, y_train)

# ===============================
# 3. Meilleur modèle et prédictions
# ===============================
print("Meilleur modèle :", grid.best_params_)

best = grid.best_estimator_
y_pred_best = best.predict(X_test)

r2 = r2_score(y_test, y_pred_best)
mse = mean_squared_error(y_test, y_pred_best)

print("R2 du meilleur modèle :", r2)
print("MSE du meilleur modèle :", mse)

# ===============================
# 4. Figure prédictions vs réel
# ===============================
plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred_best, alpha=0.5, color="skyblue")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel("Valeurs réelles")
plt.ylabel("Prédictions Random Forest")
plt.title("Prédictions vs Valeurs réelles (Meilleur Random Forest)")
plt.grid(True)
plt.show()